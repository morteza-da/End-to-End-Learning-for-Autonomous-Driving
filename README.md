# End-to-End Learning for autonomous-driving
This project presents an end-to-end deep learning model developed for decision-making in autonomous vehicles. It is based on my Ph.D. dissertation, integrating LiDAR and camera inputs to predict driving actions using CNN, LSTM, and Attention mechanisms.

## Model Overview
**Input**: LiDAR point clouds + Camera images
**Architecture**: Dual CNN (Xception & ResNeXt) â†’ LSTM + Attention
**Output**: Steering angle and speed commands
**Method**: End-to-End learning (E2E) on real-world data

## ğŸ—‚ï¸ Project Structure
â”œâ”€â”€ data/ # Sample input data (or links to it)
â”œâ”€â”€ src/ # Source code
â”œâ”€â”€ models/ # Trained models (optional or linked)
â”œâ”€â”€ notebooks/ # Jupyter notebooks for analysis
â”œâ”€â”€ results/ # Visualizations, graphs, and logs
â””â”€â”€ README.md 

 ## ğŸ“Š Dataset
- 11,000+ image frames + synchronized LiDAR data
- Real-world driving scenarios (urban & highway)
- Pre-trained backbones: ImageNet (for CNN feature extractors)

- ## âš™ï¸ Dependencies
- Python 3.10+
- TensorFlow / PyTorch
- OpenCV, NumPy, Pandas
- Matplotlib / Seaborn

- ## ğŸš€ How to Run
```bash
git clone https://github.com/yourusername/e2e-autonomous-driving.git
cd e2e-autonomous-driving
pip install -r requirements.txt
python src/train.py

## ğŸ“ˆ Results
- Training RMSE: ...
- Validation RMSE: ...
- Test RMSE: ...

## ğŸ” Innovation Highlights
- First-time combination of [Xception + ResNeXt + LSTM + Attention] in E2E driving
- Parallel CNN feature extraction from multi-modal data
- Real-world deployment-ready structure

## ğŸ§‘â€ğŸ’» Author
**[Morteza Aghamohammadi]**  
Ph.D. in Mechanical Engineering â€“ Autonomous Vehicles  
[LinkedIn](https://www.linkedin.com/in/morteza-a/)) | [Email](aghamohamadi.da@gmail.com) | [ORCID](0009-0001-4837-2298)
